{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.5. **Regularizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 과적합(overfitting) 방지와 일반화 성능 향상을 위해 사용되는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "#### $ \\hspace{0.15cm} $ ① Weight decay\n",
    "#### $ \\hspace{0.3cm} \\cdot{} $ L1 Regularization(Lasso)\n",
    "#### $ \\hspace{0.3cm} \\cdot{} $ L2 Regularization(Ridge)\n",
    "#### $ \\hspace{0.15cm} $ ② Max-norm constraints\n",
    "#### $ \\hspace{0.15cm} $ ③ Dropout\n",
    "#### $ \\hspace{0.15cm} $ ④ Early Stopping\n",
    "#### $ \\hspace{0.15cm} $ ⑤ Data Argumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **L1 Regularization(Lasso)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 비용 함수에 가중치 절댓값 놈(L1 norm)을 추가하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 적용 방식** : \n",
    "#### $ \\hspace{0.15cm} $ ① Forward propagation(Cost function)\n",
    "#### $ \\hspace{0.6cm} J(W^{[1]},\\, \\textbf{b}^{[1]},\\, \\cdots{},\\, W^{[L]},\\, \\textbf{b}^{[L]}) = \\frac{1}{m} \\displaystyle\\sum^{m}_{i=1} \\ell(a^{[L](i)},\\, y^{(i)}) + \\frac{\\lambda{}}{m} \\displaystyle\\sum^{L}_{l=1} || W^{[l]} ||_{1} $\n",
    "#### $ \\hspace{4.95cm} = \\frac{1}{m} \\displaystyle\\sum^{m}_{i=1} \\ell(a^{[L](i)},\\, y^{(i)}) + \\frac{\\lambda{}}{m} \\displaystyle\\sum^{L}_{l=1} \\displaystyle\\sum^{i=n^{[l]}}_{i=1} \\displaystyle\\sum^{n^{[l-1]}}_{k=1} |w^{[l]}_{i,\\,k}| $\n",
    "#### $ \\hspace{0.15cm} $ ② Back-propagation\n",
    "#### $ \\hspace{0.6cm} \\frac{\\partial{}J}{\\partial{}W^{[l]}} = \\frac{1}{m} \\frac{\\partial{}J}{\\partial{}Z^{[l]}} (A^{[l-1]})^{T} + \\frac{\\lambda{}}{m} \\text{sign}(W^{[l]}) \\;\\; \\text{ where } \\, \\text{sign}(w^{[l]}_{i,\\,k}) = \\begin{cases} 1, \\,\\;\\;\\;\\; w^{[l]}_{i,\\,k}>0 \\\\ 0, \\,\\;\\;\\;\\; w^{[l]}_{i,\\,k}=0 \\; \\text{(undefined)} \\\\ -1, \\;\\; w^{[l]}_{i,\\,k}<0 \\end{cases} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 효과** : 일부 가중치를 정확히 $ \\, 0 $ 으로 만듬(일부 feature는 사용하지 않음) $ \\, \\rightarrow{} $ 희소성(sparcity) 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **L2 Regularization(Ridge)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 비용 함수에 가중치 프로베니우스 놈(Frobenius;L2 norm)을 추가하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 적용 방식** :\n",
    "#### $ \\hspace{0.15cm} $ ① Forward propagation(Cost function)\n",
    "#### $ \\hspace{0.6cm} J(W^{[1]},\\, \\textbf{b}^{[1]},\\, \\cdots{},\\, W^{[L]},\\, \\textbf{b}^{[L]}) = \\frac{1}{m} \\displaystyle\\sum^{m}_{i=1} \\ell(a^{[L](i)},\\, y^{(i)}) + \\frac{\\lambda{}}{2m} \\overbrace{\\displaystyle\\sum^{L}_{l=1} || W^{[l]} ||^{2}_{F}}^{\\text{Frobenius norm}} $\n",
    "#### $ \\hspace{4.95cm} = \\frac{1}{m} \\displaystyle\\sum^{m}_{i=1} \\ell(a^{[L](i)},\\, y^{(i)}) + \\frac{\\lambda{}}{2m} \\displaystyle\\sum^{L}_{l=1} \\displaystyle\\sum^{i=n^{[l]}}_{i=1} \\displaystyle\\sum^{n^{[l-1]}}_{k=1} (w^{[l]}_{i,\\,k})^{2} $\n",
    "#### $ \\hspace{0.15cm} $ ② Back-propagation\n",
    "#### $ \\hspace{0.6cm} \\frac{\\partial{}J}{\\partial{}W^{[l]}} = \\frac{1}{m} \\frac{\\partial{}J}{\\partial{}Z^{[l]}} (A^{[l-1]})^{T} + \\frac{\\lambda{}}{m} W^{[l]} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 효과** : 모든 가중치를 작게 만듬(모든 feature를 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Max-norm constraints**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망의 각 가중치 벡터(행렬)의 놈을 특정 임계값 이하로 제한하는 정규화 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 적용 방식** :\n",
    "#### $ \\hspace{0.15cm} $ ① optimization\n",
    "#### $ \\hspace{0.6cm} W^{[l]} = \\begin{cases} \\end{cases} $ **[LATEX]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Drop Out**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델이 **학습할 때** 은닉층(hidden layer)의 각 노드에 대해 활성화/비활성화할지를 결정하는 베르누이 시행을 적용하는 방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 적용 방식** :\n",
    "#### $ \\hspace{0.15cm} $ ① Forward propagation(Activate Function)\n",
    "#### $ \\hspace{0.6cm} \\underset{n^{[l]}\\times{}m}{D^{[l]}} = \\begin{bmatrix} d^{[l](1)}_{1} & d^{[l](2)}_{1} & \\cdots{} & d^{[l](m)}_{1} \\\\ d^{[l](1)}_{2} & d^{[l](2)}_{2} & \\cdots{} & d^{[l](m)}_{2} \\\\ \\vdots{} & \\vdots{} & \\ddots{} & \\vdots{} \\\\ d^{[l](1)}_{n^{[l]}} & d^{[l](2)}_{n^{[l]}} & \\cdots{} & d^{[l](m)}_{n^{[l]}} \\end{bmatrix} \\;\\; \\text{ where } \\, d_{i,j} \\, \\sim{} \\text{Bernoulli}(p=\\text{keep-prop}) $\n",
    "#### $ \\hspace{0.6cm} \\underset{n^{[l]}\\times{}m}{A^{[l]}} = \\frac{1}{p} \\cdot{} D^{[l]} \\odot{} h(Z^{[l]}) $\n",
    "#### $ \\hspace{0.15cm} $ ② Backward propagation\n",
    "#### $ \\hspace{0.6cm} \\frac{\\partial{}J}{\\partial{}A^{[k]}} = \\frac{1}{p} \\cdot{} D^{[l]} \\odot{} (W^{[k+1]})^{T} \\cdot{} \\frac{\\partial{}J}{\\partial{}Z^{[k+1]}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`WHY?`) $ \\, \\frac{1}{p} $ 를 곱하는 이유** :\n",
    "#### $ \\hspace{0.15cm} p $ 비율만큼 출력($ A^{[l]} $) 기대값이 작아지고 역전파 때 그라디언트가 기존보다 작게 추정되어, 학습이 불안정해지기 때문에 보정 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 효과** : 특정 노드의 가중치 의존 제한(가중치 분산) $ \\, \\rightarrow{} $ 앙상블(ensemble) 효과\n",
    "#### ※ 앙상블 효과 : 무작위로 선택된 노드의 부분 집합만이 활성화되면서, 여러 다른 모델처럼 작동하는 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델의 훈련 성능이 더 이상 개선되지 않을 때 학습을 조기 종료(중단)하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 효과** : 모델의 가중치 과적합 제한"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`)** Early Stopping은 비용 최적화와 정규화를 동시에 수행하기 때문에, 이를 독립적으로 처리하기는 힘듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Data Argumentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : **[CONTENTS]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
